{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea0dae3-8419-490d-96f1-65961c7c8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import math # Transformer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcef8d2-edcd-415f-b3e7-40f96874e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "# Search GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5850393-5434-4201-a7b1-97df62d1c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_quaternions(quaternion_features):\n",
    "    \"\"\"\n",
    "    w²+x²+y²+z²=1\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    norms = np.linalg.norm(quaternion_features, axis=1, keepdims=True)\n",
    "    \n",
    "    norms[norms == 0] = 1.0\n",
    "    \n",
    "    normalized_quaternions = quaternion_features / norms\n",
    "    \n",
    "    if __debug__:\n",
    "        verification_norms = np.linalg.norm(normalized_quaternions, axis=1)\n",
    "        assert np.allclose(verification_norms, 1.0, atol=1e-6), \"四元数归一化失败\"\n",
    "    \n",
    "    return normalized_quaternions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7632f44e-e9ec-4bdc-b75f-ebdbf320907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting VIVE XYZ position features...\n",
      "Extracting Vicon XYZ position features...\n",
      "VIVE XYZ feature shape: (600000, 3)\n",
      "VIVE spatial magnitude shape: (600000,)\n",
      "Vicon XYZ feature shape: (600000, 3)\n",
      "Vicon spatial magnitude shape: (600000,)\n",
      "Data alignment length: 600000\n",
      "Tracker distribution after alignment: (array([1, 2, 3]), array([200000, 200000, 200000]))\n",
      "Final feature matrix shape: (600000, 4)\n",
      "Feature details:\n",
      "- Participant ID: 1 column\n",
      "- Speed: 1 column\n",
      "- Tracker position: 1 column\n",
      "- VIVE spatial magnitude sqrt(x²+y²+z²): 1 column\n",
      "- VIVE XYZ coordinates: 3 columns\n",
      "- Total: 7 additional features\n",
      "Generating XYZ sequence data...\n",
      "Total sequences generated: 599700\n",
      "Final training data shape:\n",
      "X_time (VIVE XYZ time series): (599700, 10, 3)\n",
      "X_feat (additional features): (599700, 4)\n",
      "y (Vicon XYZ target): (599700, 3)\n",
      "Number of sequences: 599700\n",
      "\n",
      "Tracker distribution:\n",
      "  Tracker 1: 199900 sequences\n",
      "  Tracker 2: 199900 sequences\n",
      "  Tracker 3: 199900 sequences\n"
     ]
    }
   ],
   "source": [
    "# Dataset file.\n",
    "#file_path_Kabsch = '/srv/scratch/z5548879/VIVIAN_Dataset_V1/vive_vicon_comparison_data.csv'\n",
    "file_path_Kabsch = '/srv/scratch/z5548879/VIVIAN_Dataset_V2_with orientation/position_dataset.csv'\n",
    "\n",
    "# Read Kabsch data (main dataset)\n",
    "data_kabsch = pd.read_csv(file_path_Kabsch)\n",
    "\n",
    "# Reorder data to ensure sorting by participant and timestamp\n",
    "data_kabsch = data_kabsch.sort_values(['participant_id', 'time_stamp'] if 'participant_id' in data_kabsch.columns and 'time_stamp' in data_kabsch.columns \n",
    "                       else [data_kabsch.columns[0], data_kabsch.columns[4]])\n",
    "\n",
    "data_kabsch_numpy = data_kabsch.to_numpy()\n",
    "\n",
    "# Extract basic info from Kabsch data\n",
    "participant_id = data_kabsch_numpy[:, 0]  # 8 participants\n",
    "speed = data_kabsch_numpy[:, 1]           # 0.5 1.0 1.5 2.0 m/s\n",
    "height = data_kabsch_numpy[:, 2]          # Tracker: 1, 2, 3\n",
    "direction = data_kabsch_numpy[:, 3]       # XYZ\n",
    "\n",
    "# Extract position data - separate by direction (VIVE as input, Vicon as target)\n",
    "vicon_data = data_kabsch_numpy[:, 6]  # Vicon data (target)\n",
    "vive_data = data_kabsch_numpy[:, 7]   # VIVE data (input)\n",
    "\n",
    "# Extract XYZ directional position features (including tracker information)\n",
    "def extract_xyz_features(position_data, participant_ids, speed_data, height_data, direction_data):\n",
    "    \"\"\"Extract XYZ position features for each participant, speed, and tracker\"\"\"\n",
    "    unique_participants = np.unique(participant_ids)\n",
    "    unique_speeds = np.unique(speed_data)\n",
    "    unique_heights = np.unique(height_data)\n",
    "    \n",
    "    # Calculate total number of data points\n",
    "    total_points = 0\n",
    "    for participant in unique_participants:\n",
    "        for speed_val in unique_speeds:\n",
    "            for height_val in unique_heights:  # Add tracker loop\n",
    "                mask = (participant_ids == participant) & (speed_data == speed_val) & (height_data == height_val)\n",
    "                if np.any(mask):\n",
    "                    x_mask = mask & (direction_data == 'X')\n",
    "                    total_points += np.sum(x_mask)\n",
    "    \n",
    "    # Initialize feature matrix\n",
    "    xyz_features = np.zeros((total_points, 3))\n",
    "    xyz_spatial_magnitude = np.zeros(total_points)\n",
    "    feature_participant_ids = []\n",
    "    feature_speeds = []\n",
    "    feature_heights = []  # Add heights list\n",
    "    \n",
    "    current_idx = 0\n",
    "    \n",
    "    for participant in unique_participants:\n",
    "        for speed_val in unique_speeds:\n",
    "            for height_val in unique_heights:  # Add tracker loop\n",
    "                # Get data for current participant, speed, and tracker\n",
    "                base_mask = (participant_ids == participant) & (speed_data == speed_val) & (height_data == height_val)\n",
    "                \n",
    "                if np.any(base_mask):\n",
    "                    x_mask = base_mask & (direction_data == 'X')\n",
    "                    y_mask = base_mask & (direction_data == 'Y')\n",
    "                    z_mask = base_mask & (direction_data == 'Z')\n",
    "                    \n",
    "                    x_indices = np.where(x_mask)[0]\n",
    "                    y_indices = np.where(y_mask)[0] \n",
    "                    z_indices = np.where(z_mask)[0]\n",
    "                    \n",
    "                    # Ensure XYZ data lengths are consistent\n",
    "                    min_length = min(len(x_indices), len(y_indices), len(z_indices))\n",
    "                    \n",
    "                    if min_length > 0:\n",
    "                        x_values = np.array(position_data[x_indices[:min_length]], dtype=float)\n",
    "                        y_values = np.array(position_data[y_indices[:min_length]], dtype=float)\n",
    "                        z_values = np.array(position_data[z_indices[:min_length]], dtype=float)\n",
    "                        \n",
    "                        # Store XYZ features\n",
    "                        end_idx = current_idx + min_length\n",
    "                        xyz_features[current_idx:end_idx, 0] = x_values\n",
    "                        xyz_features[current_idx:end_idx, 1] = y_values\n",
    "                        xyz_features[current_idx:end_idx, 2] = z_values\n",
    "                        \n",
    "                        # Calculate spatial magnitude\n",
    "                        spatial_magnitude = np.sqrt(x_values**2 + y_values**2 + z_values**2)\n",
    "                        xyz_spatial_magnitude[current_idx:end_idx] = spatial_magnitude\n",
    "                        \n",
    "                        # Record corresponding participant, speed, and tracker information\n",
    "                        for _ in range(min_length):\n",
    "                            feature_participant_ids.append(participant)\n",
    "                            feature_speeds.append(speed_val)\n",
    "                            feature_heights.append(height_val)  # Add tracker\n",
    "                        \n",
    "                        current_idx = end_idx\n",
    "    \n",
    "    # Truncate to actually used portion\n",
    "    xyz_features = xyz_features[:current_idx]\n",
    "    xyz_spatial_magnitude = xyz_spatial_magnitude[:current_idx]\n",
    "    \n",
    "    return (xyz_features, xyz_spatial_magnitude, \n",
    "            np.array(feature_participant_ids), \n",
    "            np.array(feature_speeds),\n",
    "            np.array(feature_heights))  # Return heights\n",
    "\n",
    "print(\"Extracting VIVE XYZ position features...\")\n",
    "vive_xyz, vive_spatial_mag, vive_participants, vive_speeds, vive_heights = extract_xyz_features(\n",
    "    vive_data, participant_id, speed, height, direction)  # Add height parameter\n",
    "\n",
    "print(\"Extracting Vicon XYZ position features...\")\n",
    "vicon_xyz, vicon_spatial_mag, vicon_participants, vicon_speeds, vicon_heights = extract_xyz_features(\n",
    "    vicon_data, participant_id, speed, height, direction)  # Add height parameter\n",
    "\n",
    "print(\"VIVE XYZ feature shape:\", vive_xyz.shape)\n",
    "print(\"VIVE spatial magnitude shape:\", vive_spatial_mag.shape)\n",
    "print(\"Vicon XYZ feature shape:\", vicon_xyz.shape)\n",
    "print(\"Vicon spatial magnitude shape:\", vicon_spatial_mag.shape)\n",
    "\n",
    "# Ensure data length alignment\n",
    "min_length = min(len(vive_xyz), len(vicon_xyz))\n",
    "print(f\"Data alignment length: {min_length}\")\n",
    "\n",
    "# Truncate to same length\n",
    "vive_xyz = vive_xyz[:min_length]\n",
    "vive_spatial_mag = vive_spatial_mag[:min_length]\n",
    "vicon_xyz = vicon_xyz[:min_length]\n",
    "vicon_spatial_mag = vicon_spatial_mag[:min_length]\n",
    "aligned_participants = vive_participants[:min_length]\n",
    "aligned_speeds = vive_speeds[:min_length]\n",
    "aligned_height = vive_heights[:min_length]  # Use heights returned from function\n",
    "\n",
    "print(f\"Tracker distribution after alignment: {np.unique(aligned_height, return_counts=True)}\")\n",
    "\n",
    "# Process categorical features\n",
    "participant_encoder = LabelEncoder()\n",
    "participant_encoded = participant_encoder.fit_transform(aligned_participants)\n",
    "participant_normalised = participant_encoded / (len(np.unique(participant_encoded)) - 1)\n",
    "\n",
    "# Standardize all numerical features\n",
    "scalers = {}\n",
    "\n",
    "# VIVE XYZ position features (3D)\n",
    "scalers['vive_xyz'] = StandardScaler()\n",
    "vive_xyz_scaled = scalers['vive_xyz'].fit_transform(vive_xyz)\n",
    "\n",
    "# VIVE spatial magnitude feature (1D)\n",
    "scalers['vive_spatial'] = StandardScaler()\n",
    "vive_spatial_scaled = scalers['vive_spatial'].fit_transform(vive_spatial_mag.reshape(-1, 1))\n",
    "\n",
    "# Vicon target variable (3D)\n",
    "scalers['vicon_xyz'] = StandardScaler()\n",
    "vicon_xyz_scaled = scalers['vicon_xyz'].fit_transform(vicon_xyz)\n",
    "\n",
    "# Vicon spatial magnitude feature (1D)\n",
    "scalers['vicon_spatial'] = StandardScaler()\n",
    "vicon_spatial_scaled = scalers['vicon_spatial'].fit_transform(vicon_spatial_mag.reshape(-1, 1))\n",
    "\n",
    "# Other features\n",
    "scalers['speed'] = StandardScaler()\n",
    "scalers['height'] = StandardScaler()\n",
    "\n",
    "speed_scaled = scalers['speed'].fit_transform(aligned_speeds.reshape(-1, 1))\n",
    "height_scaled = scalers['height'].fit_transform(aligned_height.reshape(-1, 1))\n",
    "\n",
    "# Combine all features (excluding quaternions)\n",
    "additional_features = np.hstack([\n",
    "    participant_normalised.reshape(-1, 1),    # 1 feature: participant ID\n",
    "    speed_scaled,                             # 1 feature: speed\n",
    "    height_scaled,                            # 1 feature: tracker position\n",
    "    vive_spatial_scaled,                      # 1 feature: VIVE spatial magnitude\n",
    "    #vive_xyz_scaled,                          # 3 features: VIVE XYZ coordinates\n",
    "])\n",
    "\n",
    "print(\"Final feature matrix shape:\", additional_features.shape)\n",
    "\n",
    "# Generate sequence data\n",
    "seq_length = 10\n",
    "\n",
    "def create_sequences_with_xyz_features(xyz_input, additional_feats, xyz_target, participant_ids, heights, seq_length=10):\n",
    "    \"\"\"Create sequence data with XYZ features, grouped by participant and tracker\"\"\"\n",
    "    x_time, x_feat, y, seq_participant_ids, seq_heights = [], [], [], [], []\n",
    "    \n",
    "    unique_participants = np.unique(participant_ids)\n",
    "    unique_heights = np.unique(heights)\n",
    "    \n",
    "    # Group by participant and tracker combination\n",
    "    for participant in unique_participants:\n",
    "        for height in unique_heights:\n",
    "            # Filter by both participant and tracker\n",
    "            mask = (participant_ids == participant) & (heights == height)\n",
    "            indices = np.where(mask)[0]\n",
    "            \n",
    "            if len(indices) < seq_length + 1:\n",
    "                continue\n",
    "            \n",
    "            # Create sequences for current participant-tracker combination\n",
    "            for i in range(len(indices) - seq_length):\n",
    "                # Critical modification: use indices array instead of direct indexing\n",
    "                seq_indices = indices[i:i+seq_length]  # Get sequence indices\n",
    "                target_idx = indices[i + seq_length]    # Target index\n",
    "                \n",
    "                x_time.append(xyz_input[seq_indices])        # Use sequence indices\n",
    "                x_feat.append(additional_feats[target_idx])  # Additional features\n",
    "                y.append(xyz_target[target_idx])             # XYZ target\n",
    "                seq_participant_ids.append(participant_ids[target_idx])\n",
    "                seq_heights.append(heights[target_idx])\n",
    "    \n",
    "    print(f\"Total sequences generated: {len(x_time)}\")\n",
    "    \n",
    "    return (np.array(x_time), np.array(x_feat), np.array(y), \n",
    "            np.array(seq_participant_ids), np.array(seq_heights))\n",
    "\n",
    "print(\"Generating XYZ sequence data...\")\n",
    "X_time, X_feat, y, seq_participant_ids, seq_heights = create_sequences_with_xyz_features(\n",
    "    vive_xyz_scaled, additional_features, vicon_xyz_scaled, \n",
    "    aligned_participants, aligned_height, seq_length\n",
    ")\n",
    "\n",
    "print(\"Final training data shape:\")\n",
    "print(\"X_time (VIVE XYZ time series):\", X_time.shape)\n",
    "print(\"X_feat (additional features):\", X_feat.shape)\n",
    "print(\"y (Vicon XYZ target):\", y.shape)\n",
    "print(\"Number of sequences:\", len(seq_participant_ids))\n",
    "\n",
    "# Check tracker distribution\n",
    "unique_heights_in_seq, counts = np.unique(seq_heights, return_counts=True)\n",
    "print(\"\\nTracker distribution:\")\n",
    "for h, c in zip(unique_heights_in_seq, counts):\n",
    "    print(f\"  Tracker {int(h)}: {c} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d92c41-cb5e-41ca-b3ab-e82313d8a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, min_delta=1e-5, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d557c9-b499-43a9-a7e2-41263f2ad972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Add Transformer location information\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, batch_first=True):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        if batch_first:\n",
    "            # (1, max_len, d_model) for batch_first=True\n",
    "            pe = pe.unsqueeze(0)\n",
    "        else:\n",
    "            # (max_len, 1, d_model) for batch_first=False\n",
    "            pe = pe.unsqueeze(1)\n",
    "            \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.batch_first:\n",
    "            # x shape: (batch_size, seq_len, d_model)\n",
    "            seq_len = x.size(1)\n",
    "            return x + self.pe[:, :seq_len, :]\n",
    "        else:\n",
    "            # x shape: (seq_len, batch_size, d_model)\n",
    "            seq_len = x.size(0)\n",
    "            return x + self.pe[:seq_len, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85393d67-18ae-4c85-9fff-ffe72a627abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Single Transformer block with Pre-LayerNorm architecture\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier uniform initialization\"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.constant_(self.linear1.bias, 0)\n",
    "        nn.init.constant_(self.linear2.bias, 0)\n",
    "        \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None, return_attention=False):\n",
    "        # Pre-LayerNorm:\n",
    "        # Self-attention\n",
    "        norm_src = self.norm1(src)\n",
    "        src2, attention_weights = self.self_attn(norm_src, norm_src, norm_src, \n",
    "                                               attn_mask=src_mask,\n",
    "                                               key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        # Feed forward\n",
    "        norm_src = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(norm_src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        \n",
    "        if return_attention:\n",
    "            return src, attention_weights\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86eb242-6bcf-4a5f-9403-c29b40328d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdvancedTransformerModel(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv_ms1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv_ms2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv_ms3): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (conv2): Sequential(\n",
      "    (0): GroupNorm(12, 96, eps=1e-05, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(96, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (gradient_conv): Sequential(\n",
      "    (0): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (2): Tanh()\n",
      "    (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (dropout_cnn): Dropout(p=0.09, inplace=False)\n",
      "  (cnn_projection): Linear(in_features=160, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.09, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.09, inplace=False)\n",
      "      (dropout2): Dropout(p=0.09, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (global_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (feat_fc1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_fc2): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_res): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feature_fusion): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.09, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AdvancedTransformerModel(nn.Module):\n",
    "    def __init__(self, seq_length, num_features=4, hidden_dim=128, dropout_rate=0.3,\n",
    "                 nhead=8, num_transformer_layers=3, dim_feedforward=512):\n",
    "        super(AdvancedTransformerModel, self).__init__()\n",
    "        \n",
    "        # Ensure hidden_dim is divisible by nhead\n",
    "        if hidden_dim % nhead != 0:\n",
    "            hidden_dim = ((hidden_dim // nhead) + 1) * nhead\n",
    "            print(f\"Adjusted hidden_dim to {hidden_dim} to fit multi-head attention\")\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Enhanced CNN feature extractor\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Multi-scale convolution\n",
    "        self.conv_ms1 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv_ms2 = nn.Conv1d(64, 32, kernel_size=5, padding=2)\n",
    "        self.conv_ms3 = nn.Conv1d(64, 32, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.GroupNorm(12, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(96, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(16, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Gradient feature extraction\n",
    "        self.gradient_conv = nn.Sequential(\n",
    "            nn.Conv1d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(4, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(4, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.dropout_cnn = nn.Dropout(dropout_rate * 0.3)\n",
    "        \n",
    "        # Projection layer\n",
    "        conv_output_dim = 128 + 32\n",
    "        self.cnn_projection = nn.Linear(conv_output_dim, hidden_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len=seq_length, batch_first=True)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock(hidden_dim, nhead, dim_feedforward, dropout_rate * 0.3)\n",
    "            for _ in range(num_transformer_layers)\n",
    "        ])\n",
    "        \n",
    "        # Global attention pooling layer\n",
    "        self.global_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim, \n",
    "            num_heads=nhead, \n",
    "            dropout=dropout_rate * 0.3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Learnable query vector for global pooling\n",
    "        self.global_query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        \n",
    "        # Enhanced feature processing network\n",
    "        self.feat_fc1 = nn.Sequential(\n",
    "            nn.Linear(num_features, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.3)\n",
    "        )\n",
    "        \n",
    "        self.feat_fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Improved residual connection\n",
    "        self.feat_res = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.LayerNorm(128)\n",
    "        )\n",
    "        \n",
    "        # Feature fusion layer\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 128, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.3),\n",
    "            nn.Linear(hidden_dim // 2, 3)\n",
    "        )\n",
    "        \n",
    "        # Weight initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n",
    "                if module.weight is not None:\n",
    "                    nn.init.constant_(module.weight, 1)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        \n",
    "    def forward(self, x_time, x_feat, return_attention=False):\n",
    "        batch_size = x_time.size(0)\n",
    "        \n",
    "        # Time series CNN feature extraction\n",
    "        x_time_input = x_time.permute(0, 2, 1)\n",
    "        \n",
    "        # Main feature extraction\n",
    "        x = self.conv1(x_time_input)\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        x_ms1 = self.conv_ms1(x)\n",
    "        x_ms2 = self.conv_ms2(x)\n",
    "        x_ms3 = self.conv_ms3(x)\n",
    "        x_multi_scale = torch.cat([x_ms1, x_ms2, x_ms3], dim=1)\n",
    "        \n",
    "        x = self.conv2(x_multi_scale)\n",
    "        \n",
    "        # Gradient feature extraction\n",
    "        x_grad_input = torch.diff(x_time_input, dim=2, prepend=x_time_input[:, :, :1])\n",
    "        x_grad = self.gradient_conv(x_grad_input)\n",
    "        \n",
    "        # Combine main features and gradient features\n",
    "        x_combined = torch.cat([x, x_grad], dim=1)\n",
    "        x_combined = self.dropout_cnn(x_combined)\n",
    "        \n",
    "        # Convert to Transformer input format\n",
    "        x_combined = x_combined.permute(0, 2, 1)\n",
    "        x_combined = self.cnn_projection(x_combined)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x_combined = self.pos_encoder(x_combined)\n",
    "        \n",
    "        # Pass through Transformer layers\n",
    "        attention_weights_list = []\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            if return_attention:\n",
    "                x_combined, attention_weights = transformer_layer(x_combined, return_attention=True)\n",
    "                attention_weights_list.append(attention_weights)\n",
    "            else:\n",
    "                x_combined = transformer_layer(x_combined, return_attention=False)\n",
    "        \n",
    "        # Global attention pooling\n",
    "        global_query = self.global_query.expand(batch_size, -1, -1)\n",
    "        context, global_attention_weights = self.global_attention(\n",
    "            global_query, x_combined, x_combined\n",
    "        )\n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        # Feature processing\n",
    "        feat_out = self.feat_fc1(x_feat)\n",
    "        feat_out = self.feat_fc2(feat_out)\n",
    "        feat_residual = self.feat_res(x_feat)\n",
    "        feat_out = feat_out + feat_residual\n",
    "        \n",
    "        # Feature fusion\n",
    "        combined = torch.cat([context, feat_out], dim=1)\n",
    "        fused_features = self.feature_fusion(combined)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.classifier(fused_features)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, {\n",
    "                'transformer_attention': attention_weights_list,\n",
    "                'global_attention': global_attention_weights\n",
    "            }\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = AdvancedTransformerModel(seq_length, num_features=4).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381c11a7-c567-45ec-8975-29a0c2c66050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimiser, scheduler, epochs=40, patience=8):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    history = {'train_loss': [], 'val_loss': [],\n",
    "              'train_loss_eval_mode':[]\n",
    "              }\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Gradient accumulation settings\n",
    "    accum_steps = 4  # Update weights every 4 batches\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(\"---\" * 30)  # Progress bar separator\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_predictions = []\n",
    "        train_actuals = []\n",
    "        train_losses = []\n",
    "        \n",
    "        optimiser.zero_grad()  # Zero gradients at start of epoch\n",
    "        \n",
    "        # Create progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\", leave=True)\n",
    "        \n",
    "        for i, (batch_X_time, batch_X_feat, batch_y) in enumerate(train_pbar):\n",
    "            # Move data to device\n",
    "            batch_X_time = batch_X_time.to(device)\n",
    "            batch_X_feat = batch_X_feat.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_X_time, batch_X_feat)\n",
    "            \n",
    "            # Calculate original loss for logging and statistics\n",
    "            original_loss = criterion(output, batch_y)\n",
    "            \n",
    "            # Record original loss for statistics\n",
    "            train_losses.append(original_loss.item())\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            scaled_loss = original_loss / accum_steps\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "            # Collect predictions and actuals for RMSE calculation (detach from graph)\n",
    "            train_predictions.append(output.detach().cpu().numpy())\n",
    "            train_actuals.append(batch_y.detach().cpu().numpy())\n",
    "            \n",
    "            # Update weights every accum_steps batches\n",
    "            if (i + 1) % accum_steps == 0 or (i + 1) == len(train_loader):\n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "                \n",
    "                # Clean up GPU memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Update progress bar with original loss\n",
    "            train_pbar.set_postfix({\"batch_loss\": f\"{original_loss.item():.4f}\"})\n",
    "            \n",
    "            # Free up memory\n",
    "            del batch_X_time, batch_X_feat, batch_y, output, original_loss, scaled_loss\n",
    "        \n",
    "        # Calculate training loss\n",
    "        train_loss = np.mean(train_losses)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_actuals = []     \n",
    "        train_eval_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_eval_pbar = tqdm(train_loader, desc=\"Train Eval\", leave=True)\n",
    "            for batch_X_time, batch_X_feat, batch_y in train_eval_pbar:\n",
    "                batch_X_time = batch_X_time.to(device)\n",
    "                batch_X_feat = batch_X_feat.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                output = model(batch_X_time, batch_X_feat)\n",
    "                batch_loss = criterion(output, batch_y).item()\n",
    "                train_eval_losses.append(batch_loss)\n",
    "                \n",
    "                del batch_X_time, batch_X_feat, batch_y, output\n",
    "        \n",
    "        train_eval_loss = np.mean(train_eval_losses)\n",
    "        history['train_loss_eval_mode'].append(train_eval_loss)\n",
    "\n",
    "        val_losses = []\n",
    "        # Create progress bar for validation\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", leave=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X_time, batch_X_feat, batch_y in val_pbar:\n",
    "                # Move data to device\n",
    "                batch_X_time = batch_X_time.to(device)\n",
    "                batch_X_feat = batch_X_feat.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                output = model(batch_X_time, batch_X_feat)\n",
    "                \n",
    "                # Collect predictions and actuals for RMSE calculation\n",
    "                val_predictions.append(output.cpu().numpy())\n",
    "                val_actuals.append(batch_y.cpu().numpy())\n",
    "                \n",
    "                # Calculate loss for progress bar\n",
    "                batch_loss = criterion(output, batch_y).item()\n",
    "                val_losses.append(batch_loss)\n",
    "                val_pbar.set_postfix({\"batch_loss\": f\"{batch_loss:.4f}\"})\n",
    "                \n",
    "                # Free up memory\n",
    "                del batch_X_time, batch_X_feat, batch_y, output\n",
    "        \n",
    "        # Calculate validation loss\n",
    "        val_loss = np.mean(val_losses)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed - Train Loss: {train_loss:.4f}, Train Eval: {train_eval_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        for param_group in optimiser.param_groups:\n",
    "            print(f\"Current LR: {param_group['lr']}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            print(f\"New best model saved with Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Clean up GPU memory at end of epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b36d50-2dd6-4e5e-9480-2b9d32a9f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    #val_loss = 0.0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X_time, batch_X_feat, batch_y in val_loader:\n",
    "            batch_X_time = batch_X_time.to(device)\n",
    "            batch_X_feat = batch_X_feat.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            #output = model(batch_X_time, batch_X_feat)\n",
    "            #output, attention_info = model(batch_X_time, batch_X_feat, return_attention=True)\n",
    "            if hasattr(model, 'return_attention'):\n",
    "                 output, attention_info = model(batch_X_time, batch_X_feat, return_attention=True)\n",
    "            else:\n",
    "                 output = model(batch_X_time, batch_X_feat)\n",
    "                \n",
    "            loss = criterion(output, batch_y)\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "            # Collect predictions and actuals for later analysis\n",
    "            predictions.append(output.cpu().numpy())\n",
    "            actuals.append(batch_y.cpu().numpy())\n",
    "            \n",
    "            # Free memory\n",
    "            del batch_X_time, batch_X_feat, batch_y, output, loss\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    predictions = np.vstack(predictions)\n",
    "    actuals = np.vstack(actuals)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    # avg_val_loss = val_losses / len(val_loader)\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    return avg_val_loss, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2356234a-ec88-4744-b693-bba79d6e914e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Participant-based K-fold Cross Validation...\n",
      "Available participants: ['HW3-001' 'HW3-002' 'HW3-003' 'HW3-004' 'HW3-005' 'HW3-006' 'HW3-007'\n",
      " 'HW3-008' 'HW3-009' 'HW3-010']\n"
     ]
    }
   ],
   "source": [
    "# Participant-based K-fold cross-validation\n",
    "print(\"Starting Participant-based K-fold Cross Validation...\")\n",
    "# Get unique participant IDs\n",
    "unique_participants = np.unique(seq_participant_ids)\n",
    "print(f\"Available participants: {unique_participants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c906ed-969f-4598-9a44-84ec129c58f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/10 - Validation Participant: HW3-001\n",
      "------------------------------------------------------------\n",
      "Training samples: 539730, Validation samples: 59970\n",
      "\n",
      "Model Structure for Fold 1:\n",
      "AdvancedTransformerModel(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv_ms1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv_ms2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv_ms3): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (conv2): Sequential(\n",
      "    (0): GroupNorm(12, 96, eps=1e-05, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(96, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (gradient_conv): Sequential(\n",
      "    (0): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (2): Tanh()\n",
      "    (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (dropout_cnn): Dropout(p=0.09, inplace=False)\n",
      "  (cnn_projection): Linear(in_features=160, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.09, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.09, inplace=False)\n",
      "      (dropout2): Dropout(p=0.09, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (global_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (feat_fc1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_fc2): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_res): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feature_fusion): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.09, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 1/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7a438194b24921aa215ec785769d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c9d8305d6c414182d0fc3e7bad6b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e8387537d5432c90e6d3092ab7524f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 completed - Train Loss: 0.1637, Train Eval: 0.0438, Val Loss: 0.8487\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.8487\n",
      "\n",
      "Epoch 2/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c38ce151d748c1b00352b62d03a48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdf97445a4046a29632ba23e1f93c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6131a4c6585468f979181220ba91ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 completed - Train Loss: 0.0580, Train Eval: 0.0221, Val Loss: 0.8340\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.8340\n",
      "\n",
      "Epoch 3/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890f867d82a94616906a2f89dfe1ff02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2808fad033a34c7e8650f2fea31e9e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363e81ff400547b4903d99943a7b761f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 completed - Train Loss: 0.0412, Train Eval: 0.0154, Val Loss: 0.8436\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 4/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bce13b65c2429fb930568f6fae4299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e2ed9f66684743a7de086cc6ba0212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463ff7a2ba5e4078a3bc4e25910b1928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 completed - Train Loss: 0.0357, Train Eval: 0.0130, Val Loss: 0.8146\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.8146\n",
      "\n",
      "Epoch 5/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc6aa6fd7cd4357abc1c4d2ef824425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1fdb039b974dd985830bc0f5bb26e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7463cd531542d7af30605022f0ed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 completed - Train Loss: 0.0329, Train Eval: 0.0124, Val Loss: 0.8394\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 6/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae9211ad6aa4938b431239bda676991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41540ec61954bfb91345704fc408e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cbcba75f3c4e55aba453b0976c9019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 completed - Train Loss: 0.0314, Train Eval: 0.0112, Val Loss: 0.8119\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.8119\n",
      "\n",
      "Epoch 7/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de76b2539aa4d3a877e494f7b81a344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d706d49724ae896c31383da62cfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b314782820464951a8a63cf5060f91c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 completed - Train Loss: 0.0302, Train Eval: 0.0113, Val Loss: 0.7941\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.7941\n",
      "\n",
      "Epoch 8/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342b8db251b4449a888487ca8ec23f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764d1a01d85b4889a7d019ccc889fc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52e54700a2d47be904c5f0e882857d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 completed - Train Loss: 0.0290, Train Eval: 0.0100, Val Loss: 0.9089\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 9/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850cfd763c7a43f5890c6e8c7f982168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35536c8892204b22879dbcf8ced013cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affbd376446d4803b33af2aa03c20d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 completed - Train Loss: 0.0284, Train Eval: 0.0101, Val Loss: 0.8389\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 10/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7d59975c0d450aacf85561ba7ccca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a3f5557243488d9ae322e46c3644cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa879d6c78bf4c6abfaa2d49812f22e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 completed - Train Loss: 0.0278, Train Eval: 0.0096, Val Loss: 0.8754\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 11/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451de919efba4f0e94e11e924006307b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aeb96a550643e191807bd4b14c0b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c63a6afd0294d528b044c83a964bf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 completed - Train Loss: 0.0273, Train Eval: 0.0095, Val Loss: 0.8757\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 12/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2d27843fa54bc08f37c54a9f76bd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44499ddd7d7141eabb5ba3c014e048e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ab655fbb48433b9cc65fa6e19264db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 completed - Train Loss: 0.0258, Train Eval: 0.0082, Val Loss: 0.8909\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 13/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f40e883b0440b0a8df18c1c1c933af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fabb342a534d37a999409f9d4a2806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7024aaa352f94b5bb8a010c9e1af98ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 completed - Train Loss: 0.0255, Train Eval: 0.0083, Val Loss: 0.8329\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 14/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5383e0a46341fab050335e66827368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a596161f570f4c63af37ab96500f8de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2788b40c0f004833b76c4d0a15275824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 completed - Train Loss: 0.0253, Train Eval: 0.0087, Val Loss: 0.8428\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 15/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eead591db5d14255813af805a157f1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a63077c12b4207ad5c8f34f4b20b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8957d48ca074c60b9bcc97936082839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 completed - Train Loss: 0.0252, Train Eval: 0.0078, Val Loss: 0.8586\n",
      "Current LR: 0.00025\n",
      "Early stopping triggered at epoch 15\n",
      "Fold 1 (HW3-001) Loss: 0.8586\n",
      "\n",
      "Fold 2/10 - Validation Participant: HW3-002\n",
      "------------------------------------------------------------\n",
      "Training samples: 539730, Validation samples: 59970\n",
      "\n",
      "Model Structure for Fold 2:\n",
      "AdvancedTransformerModel(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv_ms1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv_ms2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv_ms3): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (conv2): Sequential(\n",
      "    (0): GroupNorm(12, 96, eps=1e-05, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(96, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (gradient_conv): Sequential(\n",
      "    (0): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (2): Tanh()\n",
      "    (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (dropout_cnn): Dropout(p=0.09, inplace=False)\n",
      "  (cnn_projection): Linear(in_features=160, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.09, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.09, inplace=False)\n",
      "      (dropout2): Dropout(p=0.09, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (global_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (feat_fc1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_fc2): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_res): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feature_fusion): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.09, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 1/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84c7d09c964acd98763f0642dce7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde998f0702b43d49353db418420ccd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a43c91e73324de49b913c701ea458cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 completed - Train Loss: 0.1797, Train Eval: 0.0528, Val Loss: 0.3991\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.3991\n",
      "\n",
      "Epoch 2/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bc873625654058947ecc0fd4617a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e419a1c2010c40fc899d914caa9557ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7ee5fffa774c84829a7e9014e6da43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 completed - Train Loss: 0.0632, Train Eval: 0.0253, Val Loss: 0.4609\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 3/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccba02a83e8b496e9cfadda398431fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0381c4f83fc04547abf47eefc2fc17ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35425ceb41644c78b7388e2135ae99a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 completed - Train Loss: 0.0439, Train Eval: 0.0182, Val Loss: 0.4510\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 4/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e3268cac2c48f5b3f5b07a7a68d82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c328115fffec48cc8082d914e62ca13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f37bcdbadc34b85b76e80647224b43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 completed - Train Loss: 0.0371, Train Eval: 0.0141, Val Loss: 0.4698\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 5/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c39897a3394fc5890250e42872c685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a79180176b047bbbdb5493d7bf35cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c4fc6c4ce94608a78c63a0a96d25ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 completed - Train Loss: 0.0338, Train Eval: 0.0137, Val Loss: 0.5013\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 6/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2641cbb71900429ea20298cef9afeadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b719f18988a4678a0921ebeb8fb5beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e8b6d72ef341deae014fc21399337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 completed - Train Loss: 0.0308, Train Eval: 0.0108, Val Loss: 0.4725\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 7/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa465c02b7f43be9d5ca43203f45ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e68775870034083a4c4a30aca844caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ffe61fe3e460e9518e24c016059d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 completed - Train Loss: 0.0300, Train Eval: 0.0098, Val Loss: 0.4775\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 8/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79e8b1d78cd4da4bad8073898181476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6a9b5b512040478627098a1a22488d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7032e4ff4c44a1ba16104755169416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 completed - Train Loss: 0.0294, Train Eval: 0.0094, Val Loss: 0.5001\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 9/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223c3f296c694d48a98239bb0620e0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeb6f8a4cde4a428209164a1ef54488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c8f5e34dfe4b61b22dddc281cf406d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 completed - Train Loss: 0.0288, Train Eval: 0.0095, Val Loss: 0.4636\n",
      "Current LR: 0.00025\n",
      "Early stopping triggered at epoch 9\n",
      "Fold 2 (HW3-002) Loss: 0.4636\n",
      "\n",
      "Fold 3/10 - Validation Participant: HW3-003\n",
      "------------------------------------------------------------\n",
      "Training samples: 539730, Validation samples: 59970\n",
      "\n",
      "Model Structure for Fold 3:\n",
      "AdvancedTransformerModel(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv_ms1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv_ms2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv_ms3): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (conv2): Sequential(\n",
      "    (0): GroupNorm(12, 96, eps=1e-05, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(96, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (gradient_conv): Sequential(\n",
      "    (0): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (2): Tanh()\n",
      "    (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (dropout_cnn): Dropout(p=0.09, inplace=False)\n",
      "  (cnn_projection): Linear(in_features=160, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.09, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.09, inplace=False)\n",
      "      (dropout2): Dropout(p=0.09, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (global_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (feat_fc1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_fc2): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_res): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feature_fusion): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.09, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 1/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d801f818e81467ca984382888ed765b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a53063730a74be796482a40aa2eed77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01bc9e09f2847339f42d197f3162291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 completed - Train Loss: 0.1690, Train Eval: 0.0435, Val Loss: 0.3107\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.3107\n",
      "\n",
      "Epoch 2/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cf77cf122241e3af5d8c9172bb6407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb88f55fbded41829237f44b66fffc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661d5fa0cd364a24b62dcfbcbaf1172c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 completed - Train Loss: 0.0592, Train Eval: 0.0212, Val Loss: 0.2518\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.2518\n",
      "\n",
      "Epoch 3/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c325acda2046efb9e310819f8fcd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dcd2f7e12846cca822aa44242668df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c08dff0161f42219f675842343b3d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 completed - Train Loss: 0.0422, Train Eval: 0.0172, Val Loss: 0.2596\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 4/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486337879fd646b19a1e338f842ffcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f22b677a804d6c9f84eaaafac04ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e88de81934a4a8cb907618c9204a1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 completed - Train Loss: 0.0366, Train Eval: 0.0137, Val Loss: 0.2681\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 5/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5607d36dd9140038acb2e0f804cfa6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de146eb51874b1fbb8e62e787589089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b89907c52f40faaf817e4ed48c1959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 completed - Train Loss: 0.0337, Train Eval: 0.0124, Val Loss: 0.2518\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.2518\n",
      "\n",
      "Epoch 6/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784eaf8123ae4f4eb865855713138d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410ac13ac2904cb78009bbb494e3ea31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48a9558f89241a68043d6df7025e0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 completed - Train Loss: 0.0318, Train Eval: 0.0117, Val Loss: 0.2607\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 7/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab451de54a64f8a93f846fa500f853d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9536544bec94c2a9becf88001ac1e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99efde2116ea4ecda01585c52ed53daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 completed - Train Loss: 0.0304, Train Eval: 0.0108, Val Loss: 0.2837\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 8/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea51d67b80e74eadb291ffef1d17edd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0a63236d6f40b08961b01deeffb034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2818e7a75cb94e1a9fca82c936ea5928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 completed - Train Loss: 0.0294, Train Eval: 0.0107, Val Loss: 0.2359\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.2359\n",
      "\n",
      "Epoch 9/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b4433530894c368f2f4c42d1a8f1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fb2be865554052834bd641ec71ac3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3f9c9ede5c479481339d73ecbfbbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 completed - Train Loss: 0.0288, Train Eval: 0.0126, Val Loss: 0.2740\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 10/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a90d8289bf4d67be11db4529c060cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d63c719175d49788e188bfbad2dc155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42872a7b04cf4ba8a22613e83a4ca0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 completed - Train Loss: 0.0282, Train Eval: 0.0100, Val Loss: 0.2555\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 11/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43494147e2b245679afce647244fca7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4876e4559f448097bf004f9fc9b7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab09bec594842bfb67982a8dcf4b1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 completed - Train Loss: 0.0277, Train Eval: 0.0099, Val Loss: 0.2485\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 12/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e821c50eab794824a852d311ef40813d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60186cab6a724c58bbf9a52e3fced603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adee51c7ad0b46cdb1e93f0ca05dec07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 completed - Train Loss: 0.0272, Train Eval: 0.0092, Val Loss: 0.2373\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 13/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b2d4b9098f4aada28d90195d3e87dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59bcda8422344f287e71d6b122e2162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f37dc0c6505487e94794e99b1c15eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 completed - Train Loss: 0.0256, Train Eval: 0.0076, Val Loss: 0.2394\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 14/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67843fad4d1d494ea9db3133c2b2ffa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cb6e5842a74277a17481fe35d47ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9eea08c9a34c9993eeb9d95490a544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 completed - Train Loss: 0.0254, Train Eval: 0.0085, Val Loss: 0.2384\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 15/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbc5cd92e2740ad91b446e9233f55b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35dc417fcf444e491553f2c33dbd4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76804081f54d4801a560bde5a90095b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 completed - Train Loss: 0.0253, Train Eval: 0.0076, Val Loss: 0.2204\n",
      "Current LR: 0.0005\n",
      "New best model saved with Val Loss: 0.2204\n",
      "\n",
      "Epoch 16/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67607c547fe7496cba59023bc142b9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7580f35b84243c59cdad091e220b79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac40b3c024544b3b3f1ea8eb4ae535a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 completed - Train Loss: 0.0251, Train Eval: 0.0078, Val Loss: 0.2484\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 17/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4de1cf8c17949c1a43f9d4939c38128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52499395601942f4baa7999a40b35160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777c1d7838a0482c9ad4aebf3c293542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 completed - Train Loss: 0.0249, Train Eval: 0.0078, Val Loss: 0.2344\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 18/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b952cb692c4a438250d55c499e211c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df4194e1f9a4eaabe278d5c4f9536b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a4860d93dd46f7b8b4b0c857030207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 completed - Train Loss: 0.0249, Train Eval: 0.0078, Val Loss: 0.2165\n",
      "Current LR: 0.0005\n",
      "New best model saved with Val Loss: 0.2165\n",
      "\n",
      "Epoch 19/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51defd165a347adb244a322ec5ddf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ef5164345140eca3ed76b0ab7e0905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf90994b590c43199558b7e3fe69ee8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 completed - Train Loss: 0.0246, Train Eval: 0.0075, Val Loss: 0.2213\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 20/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e04f2e0e12e42248f9a6e35e7fa4077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673dfc15c5d34122ba8a26a8dda1e089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c22400812949af8edd8c0814ea9046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 completed - Train Loss: 0.0246, Train Eval: 0.0074, Val Loss: 0.2677\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 21/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ca171b6a504205a5f0eb40dad37af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2921e635fef246d5ac3f763d32d11f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23748e67ea654f86a4b6474fbf20dfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 completed - Train Loss: 0.0243, Train Eval: 0.0074, Val Loss: 0.2459\n",
      "Current LR: 0.0005\n",
      "\n",
      "Epoch 22/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd6d9c3dc01404a939652e2b69b0485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34d5ad9fe6b4487a854ee1a41597bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22922e7cabb40dab86c98489e2ee6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 completed - Train Loss: 0.0242, Train Eval: 0.0072, Val Loss: 0.2366\n",
      "Current LR: 0.00025\n",
      "\n",
      "Epoch 23/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367ed86bc8f4468fa451e139ff2faae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4490a7e2f7b4c049c4a534e2b6306bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a0f33418514b16a257643b026808d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 completed - Train Loss: 0.0233, Train Eval: 0.0062, Val Loss: 0.2286\n",
      "Current LR: 0.00025\n",
      "\n",
      "Epoch 24/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c20a9365394d3287ee835a99ae094e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d4e47ecb5f4fe897b0698b02575954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360dc22838294b43ba65a90c05ea832e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 completed - Train Loss: 0.0231, Train Eval: 0.0061, Val Loss: 0.2380\n",
      "Current LR: 0.00025\n",
      "\n",
      "Epoch 25/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1a29d00c324f219c16a47c75f1cb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825d41af06847cb90c07e02384df153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7873f39aa0d2497397ffc668e4eea3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 completed - Train Loss: 0.0230, Train Eval: 0.0057, Val Loss: 0.2576\n",
      "Current LR: 0.00025\n",
      "\n",
      "Epoch 26/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe092c4d036045f3abb9c8502e6014c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4452c4ab8054991a7c156b901c44edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c744415042e4bc9afa3130b71059433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 completed - Train Loss: 0.0230, Train Eval: 0.0060, Val Loss: 0.2375\n",
      "Current LR: 0.000125\n",
      "Early stopping triggered at epoch 26\n",
      "Fold 3 (HW3-003) Loss: 0.2375\n",
      "\n",
      "Fold 4/10 - Validation Participant: HW3-004\n",
      "------------------------------------------------------------\n",
      "Training samples: 539730, Validation samples: 59970\n",
      "\n",
      "Model Structure for Fold 4:\n",
      "AdvancedTransformerModel(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv_ms1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv_ms2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv_ms3): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (conv2): Sequential(\n",
      "    (0): GroupNorm(12, 96, eps=1e-05, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(96, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (gradient_conv): Sequential(\n",
      "    (0): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (2): Tanh()\n",
      "    (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (dropout_cnn): Dropout(p=0.09, inplace=False)\n",
      "  (cnn_projection): Linear(in_features=160, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.09, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.09, inplace=False)\n",
      "      (dropout2): Dropout(p=0.09, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (global_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (feat_fc1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_fc2): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "  )\n",
      "  (feat_res): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feature_fusion): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.09, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.09, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 1/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e2b1dcb8f941daadd69b5706dd7186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a324a66fce40e98a3d589adeef6985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172e97114c2748889e019a4d6229539a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 completed - Train Loss: 0.1985, Train Eval: 0.0573, Val Loss: 0.0891\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.0891\n",
      "\n",
      "Epoch 2/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d693bb8b0f7543ba82c09ea948d1e3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410925c21c2e4197a51442591ed9081d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5e9939171c44c5a918160cb6c8aa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 completed - Train Loss: 0.0654, Train Eval: 0.0236, Val Loss: 0.0669\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.0669\n",
      "\n",
      "Epoch 3/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b4f5e288ec4d079a01066064acdb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ad4d0ddbb041eebe3ebcbe338b31d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759870eeb1ba4a4bbe6c1ea84686326e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 completed - Train Loss: 0.0445, Train Eval: 0.0194, Val Loss: 0.0503\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.0503\n",
      "\n",
      "Epoch 4/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e544d4a4a622488cb707b8979568044f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efab877acdc49abae2180c28be299b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Eval:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c853e80317154bd4b558121bfa0fa62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 completed - Train Loss: 0.0382, Train Eval: 0.0161, Val Loss: 0.0361\n",
      "Current LR: 0.001\n",
      "New best model saved with Val Loss: 0.0361\n",
      "\n",
      "Epoch 5/40\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85279e754b6848468f834e0e86b3fe52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Participant-based K-Fold Cross Validation\n",
    "num_folds = len(unique_participants)  # 10-fold (each participant as validation set once)\n",
    "fold_rmse_scores = []\n",
    "best_models = []\n",
    "all_histories = []\n",
    "batch_size = 512\n",
    "\n",
    "# Loop through each participant as validation set\n",
    "for fold, val_participant in enumerate(unique_participants):\n",
    "    print(f\"\\nFold {fold+1}/{num_folds} - Validation Participant: {val_participant}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create training and validation indices\n",
    "    val_indices = np.where(seq_participant_ids == val_participant)[0]\n",
    "    train_indices = np.where(seq_participant_ids != val_participant)[0]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "    \n",
    "    # Create training and validation sets\n",
    "    X_time_train, X_time_val = X_time[train_indices], X_time[val_indices]\n",
    "    X_feat_train, X_feat_val = X_feat[train_indices], X_feat[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_time_train, dtype=torch.float32),\n",
    "        torch.tensor(X_feat_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_time_val, dtype=torch.float32),\n",
    "        torch.tensor(X_feat_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AdvancedTransformerModel(seq_length, num_features=X_feat.shape[1]).to(device)\n",
    "    \n",
    "    print(f\"\\nModel Structure for Fold {fold+1}:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Combined MSE and L1 loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "    criterion = lambda output, target: mse_loss(output, target) + 0.1 * l1_loss(output, target)\n",
    "    \n",
    "    # Initialize optimizer and learning rate scheduler\n",
    "    optimiser = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # Train model with early stopping\n",
    "    model, history = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        criterion, \n",
    "        optimiser, \n",
    "        scheduler, \n",
    "        epochs=40, \n",
    "        patience=8\n",
    "    )\n",
    "    \n",
    "    # Evaluate model on validation set\n",
    "    fold_rmse, predictions, actuals = evaluate_model(model, val_loader, criterion)\n",
    "    fold_rmse_scores.append(fold_rmse)\n",
    "    \n",
    "    print(f\"Fold {fold+1} ({val_participant}) Loss: {fold_rmse:.4f}\")\n",
    "    \n",
    "    # Save model state and training history\n",
    "    best_models.append(model.state_dict().copy())\n",
    "    all_histories.append(history)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Print overall cross-validation results\n",
    "print(\"\\nParticipant-based K-fold Cross Validation Results:\")\n",
    "print(\"-\" * 60)\n",
    "for fold, (participant, rmse) in enumerate(zip(unique_participants, fold_rmse_scores)):\n",
    "    print(f\"Fold {fold+1} ({participant}): Loss = {rmse:.4f}\")\n",
    "print(f\"Average Loss: {np.mean(fold_rmse_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(fold_rmse_scores):.4f}\")\n",
    "\n",
    "# Select best model based on lowest validation loss\n",
    "best_fold_idx = np.argmin(fold_rmse_scores)\n",
    "best_participant = unique_participants[best_fold_idx]\n",
    "print(f\"\\nBest model is from Fold {best_fold_idx+1} (validation on {best_participant}) with Loss: {fold_rmse_scores[best_fold_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d4f04-1822-4a6a-99f8-e2340f75730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the final model with the best weights\n",
    "final_model = AdvancedTransformerModel(seq_length, num_features=X_feat.shape[1]).to(device)\n",
    "final_model.load_state_dict(best_models[best_fold_idx])\n",
    "\n",
    "# Plot training history for the best fold\n",
    "best_history = all_histories[best_fold_idx]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(best_history['train_loss'], label='Train Loss')\n",
    "plt.plot(best_history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Best Fold (#{best_fold_idx+1}) Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot average training history across all folds\n",
    "plt.subplot(1, 2, 2)\n",
    "max_epochs = max(len(h['train_loss']) for h in all_histories)\n",
    "padded_train_losses = []\n",
    "padded_val_losses = []\n",
    "\n",
    "for h in all_histories:\n",
    "    train_loss = h['train_loss'] + [h['train_loss'][-1]] * (max_epochs - len(h['train_loss']))\n",
    "    val_loss = h['val_loss'] + [h['val_loss'][-1]] * (max_epochs - len(h['val_loss']))\n",
    "    \n",
    "    padded_train_losses.append(train_loss)\n",
    "    padded_val_losses.append(val_loss)\n",
    "\n",
    "avg_train_loss = np.mean(padded_train_losses, axis=0)\n",
    "avg_val_loss = np.mean(padded_val_losses, axis=0)\n",
    "\n",
    "plt.plot(avg_train_loss, label='Avg Train Loss')\n",
    "plt.plot(avg_val_loss, label='Avg Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Average Training History Across All Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ensemble prediction function (using all trained models)\n",
    "def ensemble_predict(models, X_time_test, X_feat_test, batch_size=512):\n",
    "    predictions = []\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_time_test, dtype=torch.float32),\n",
    "        torch.tensor(X_feat_test, dtype=torch.float32)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Predict with each model\n",
    "    for model_state in models:\n",
    "        model = AdvancedTransformerModel(seq_length, num_features=X_feat.shape[1]).to(device)\n",
    "        model.load_state_dict(model_state)\n",
    "        model.eval()\n",
    "        \n",
    "        fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_time, batch_X_feat in test_loader:\n",
    "                batch_X_time = batch_X_time.to(device)\n",
    "                batch_X_feat = batch_X_feat.to(device)\n",
    "                \n",
    "                output = model(batch_X_time, batch_X_feat)\n",
    "                fold_preds.append(output.cpu().numpy())\n",
    "                \n",
    "                # Free memory\n",
    "                del batch_X_time, batch_X_feat, output\n",
    "        \n",
    "        # Concatenate predictions for this fold\n",
    "        fold_preds = np.vstack(fold_preds)\n",
    "        predictions.append(fold_preds)\n",
    "    \n",
    "    # Average predictions from all models\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return ensemble_predictions\n",
    "\n",
    "# Function to evaluate on the entire dataset using the ensemble\n",
    "def evaluate_ensemble(models, X_time, X_feat, y, batch_size=512):\n",
    "    # Get ensemble predictions\n",
    "    predictions = ensemble_predict(models, X_time, X_feat, batch_size)\n",
    "    \n",
    "    # Inverse transform predictions and targets back to original scale\n",
    "    # Updated to use the scalers dictionary\n",
    "    predictions = scalers['vicon_xyz'].inverse_transform(predictions)\n",
    "    \n",
    "    # For VIVE original, we need to inverse transform the last timestep of X_time\n",
    "    # X_time contains scaled VIVE XYZ data, so we use vive_xyz scaler\n",
    "    vive_original = scalers['vive_xyz'].inverse_transform(X_time[:, -1])  # Shape: (samples, 3)\n",
    "    \n",
    "    # Inverse transform target y\n",
    "    y_original = scalers['vicon_xyz'].inverse_transform(y)\n",
    "    \n",
    "    # Calculate metrics for Ensemble vs Vicon\n",
    "    ensemble_mse = np.mean((predictions - y_original) ** 2)\n",
    "    ensemble_rmse = np.sqrt(ensemble_mse)\n",
    "    ensemble_mae = np.mean(np.abs(predictions - y_original))\n",
    "    \n",
    "    # Calculate metrics for Original VIVE vs Vicon\n",
    "    vive_mse = np.mean((vive_original - y_original) ** 2)\n",
    "    vive_rmse = np.sqrt(vive_mse)\n",
    "    vive_mae = np.mean(np.abs(vive_original - y_original))\n",
    "    \n",
    "    # Calculate improvement\n",
    "    rmse_improvement = ((vive_rmse - ensemble_rmse) / vive_rmse) * 100\n",
    "    mae_improvement = ((vive_mae - ensemble_mae) / vive_mae) * 100\n",
    "    \n",
    "    print(f\"Model Performance Comparison:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Original VIVE vs Vicon:\")\n",
    "    print(f\"  RMSE: {vive_rmse:.4f}\")\n",
    "    print(f\"  MAE:  {vive_mae:.4f}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Ensemble Model vs Vicon:\")\n",
    "    print(f\"  RMSE: {ensemble_rmse:.4f}\")\n",
    "    print(f\"  MAE:  {ensemble_mae:.4f}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Improvement:\")\n",
    "    print(f\"  RMSE: {rmse_improvement:+.2f}% {'(Better)' if rmse_improvement > 0 else '(Worse)'}\")\n",
    "    print(f\"  MAE:  {mae_improvement:+.2f}% {'(Better)' if mae_improvement > 0 else '(Worse)'}\")\n",
    "    \n",
    "    # Visualize predictions vs actual\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot a sample of the data (first 4000 points)\n",
    "    sample_size = min(4000, len(predictions))\n",
    "    \n",
    "    # Time series comparison - show each XYZ dimension\n",
    "    for i, dim in enumerate(['X', 'Y', 'Z']):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.plot(y_original[:sample_size, i], label=f'Actual Vicon {dim}', linewidth=2)\n",
    "        plt.plot(predictions[:sample_size, i], label=f'Predicted {dim}', alpha=0.8)\n",
    "        plt.plot(vive_original[:sample_size, i], label=f'Original VIVE {dim}', alpha=0.7)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel(f'{dim} Value')\n",
    "        plt.title(f'{dim} Dimension Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # Overall correlation plot for Ensemble\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.scatter(y_original.flatten(), predictions.flatten(), alpha=0.5, s=1)\n",
    "    plt.plot([y_original.min(), y_original.max()], [y_original.min(), y_original.max()], 'r--')\n",
    "    plt.xlabel('Actual (Vicon)')\n",
    "    plt.ylabel('Predicted (Ensemble)')\n",
    "    plt.title(f'Ensemble Correlation (RMSE: {ensemble_rmse:.4f})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Overall correlation plot for Original VIVE\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.scatter(y_original.flatten(), vive_original.flatten(), alpha=0.5, s=1, color='orange')\n",
    "    plt.plot([y_original.min(), y_original.max()], [y_original.min(), y_original.max()], 'r--')\n",
    "    plt.xlabel('Actual (Vicon)')\n",
    "    plt.ylabel('Original (VIVE)')\n",
    "    plt.title(f'VIVE Correlation (RMSE: {vive_rmse:.4f})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Error distribution comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    ensemble_errors = predictions.flatten() - y_original.flatten()\n",
    "    vive_errors = vive_original.flatten() - y_original.flatten()\n",
    "    \n",
    "    plt.hist(ensemble_errors, bins=50, alpha=0.7, label=f'Ensemble (std: {np.std(ensemble_errors):.4f})', density=True)\n",
    "    plt.hist(vive_errors, bins=50, alpha=0.7, label=f'VIVE (std: {np.std(vive_errors):.4f})', density=True)\n",
    "    plt.xlabel('Error (Predicted - Actual)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Error Distribution Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\nDetailed Error Statistics:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Ensemble Model Errors:\")\n",
    "    print(f\"  Mean Error: {np.mean(ensemble_errors):.4f}\")\n",
    "    print(f\"  Std Error:  {np.std(ensemble_errors):.4f}\")\n",
    "    print(f\"  Min Error:  {np.min(ensemble_errors):.4f}\")\n",
    "    print(f\"  Max Error:  {np.max(ensemble_errors):.4f}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Original VIVE Errors:\")\n",
    "    print(f\"  Mean Error: {np.mean(vive_errors):.4f}\")\n",
    "    print(f\"  Std Error:  {np.std(vive_errors):.4f}\")\n",
    "    print(f\"  Min Error:  {np.min(vive_errors):.4f}\")\n",
    "    print(f\"  Max Error:  {np.max(vive_errors):.4f}\")\n",
    "    \n",
    "    # Print per-dimension statistics\n",
    "    print(f\"\\nPer-Dimension RMSE:\")\n",
    "    print(f\"{'='*30}\")\n",
    "    for i, dim in enumerate(['X', 'Y', 'Z']):\n",
    "        ensemble_rmse_dim = np.sqrt(np.mean((predictions[:, i] - y_original[:, i]) ** 2))\n",
    "        vive_rmse_dim = np.sqrt(np.mean((vive_original[:, i] - y_original[:, i]) ** 2))\n",
    "        improvement_dim = ((vive_rmse_dim - ensemble_rmse_dim) / vive_rmse_dim) * 100\n",
    "        \n",
    "        print(f\"{dim} Dimension:\")\n",
    "        print(f\"  VIVE RMSE:     {vive_rmse_dim:.4f}\")\n",
    "        print(f\"  Ensemble RMSE: {ensemble_rmse_dim:.4f}\")\n",
    "        print(f\"  Improvement:   {improvement_dim:+.2f}%\")\n",
    "    \n",
    "    return predictions, y_original, vive_original\n",
    "\n",
    "# Evaluate the ensemble on the entire dataset\n",
    "predictions, actuals, vive_original = evaluate_ensemble(best_models, X_time, X_feat, y)\n",
    "\n",
    "# Save the best model\n",
    "torch.save(final_model.state_dict(), 'best_vive_to_vicon_model.pth')\n",
    "print(\"Best model saved to 'best_vive_to_vicon_model.pth'\")\n",
    "\n",
    "# Feature importance analysis\n",
    "def analyze_feature_importance(model, X_time, X_feat, y, feature_names):\n",
    "    # Base performance\n",
    "    model.eval()\n",
    "    base_preds = []\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X_time, dtype=torch.float32),\n",
    "        torch.tensor(X_feat, dtype=torch.float32)\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X_time, batch_X_feat in data_loader:\n",
    "            batch_X_time = batch_X_time.to(device)\n",
    "            batch_X_feat = batch_X_feat.to(device)\n",
    "            \n",
    "            output = model(batch_X_time, batch_X_feat)\n",
    "            base_preds.append(output.cpu().numpy())\n",
    "    \n",
    "    base_preds = np.vstack(base_preds)\n",
    "    base_mse = np.mean((base_preds - y) ** 2)  # Updated: y is now 3D (samples, 3)\n",
    "    \n",
    "    # Permutation importance\n",
    "    importance_scores = []\n",
    "    \n",
    "    for i in range(X_feat.shape[1]):\n",
    "        # Create a copy and permute one feature\n",
    "        X_feat_permuted = X_feat.copy()\n",
    "        X_feat_permuted[:, i] = np.random.permutation(X_feat_permuted[:, i])\n",
    "        \n",
    "        # Predict with permuted feature\n",
    "        perm_preds = []\n",
    "        \n",
    "        dataset_perm = TensorDataset(\n",
    "            torch.tensor(X_time, dtype=torch.float32),\n",
    "            torch.tensor(X_feat_permuted, dtype=torch.float32)\n",
    "        )\n",
    "        loader_perm = DataLoader(dataset_perm, batch_size=512, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X_time, batch_X_feat in loader_perm:\n",
    "                batch_X_time = batch_X_time.to(device)\n",
    "                batch_X_feat = batch_X_feat.to(device)\n",
    "                \n",
    "                output = model(batch_X_time, batch_X_feat)\n",
    "                perm_preds.append(output.cpu().numpy())\n",
    "        \n",
    "        perm_preds = np.vstack(perm_preds)\n",
    "        perm_mse = np.mean((perm_preds - y) ** 2)  # Updated: y is now 3D (samples, 3)\n",
    "        \n",
    "        # Importance is the increase in error\n",
    "        importance = perm_mse - base_mse\n",
    "        importance_scores.append(importance)\n",
    "    \n",
    "    # Visualise feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sorted_idx = np.argsort(importance_scores)\n",
    "    plt.barh(range(len(feature_names)), [importance_scores[i] for i in sorted_idx])\n",
    "    plt.yticks(range(len(feature_names)), [feature_names[i] for i in sorted_idx])\n",
    "    plt.xlabel('Increase in MSE when feature is permuted')\n",
    "    plt.title('Feature Importance Analysis')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_scores\n",
    "\n",
    "# Define feature names for visualization (updated to match new 7 features)\n",
    "feature_names = [\n",
    "    'Participant ID', \n",
    "    'Speed', \n",
    "    'Tracker Height',\n",
    "    'VIVE Spatial Magnitude'\n",
    "]\n",
    "\n",
    "# Verify feature count matches\n",
    "assert len(feature_names) == X_feat.shape[1], f\"Feature names count ({len(feature_names)}) doesn't match actual features ({X_feat.shape[1]})\"\n",
    "\n",
    "# Analyze feature importance\n",
    "importance_scores = analyze_feature_importance(final_model, X_time, X_feat, y, feature_names)\n",
    "print(\"Feature importance analysis complete!\")\n",
    "\n",
    "# Function to save ensemble models\n",
    "def save_ensemble_models(models, model_dir='saved_models'):\n",
    "    \"\"\"\n",
    "    Save all models in the ensemble to disk\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : list\n",
    "        List of model state dictionaries\n",
    "    model_dir : str\n",
    "        Directory to save models in\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model_path : str\n",
    "        Path to the saved ensemble\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Create timestamp for unique filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    ensemble_path = os.path.join(model_dir, f\"vive_vicon_CoM_ensemble_{timestamp}\")\n",
    "    os.makedirs(ensemble_path, exist_ok=True)\n",
    "    \n",
    "    # Save each model in the ensemble\n",
    "    for i, model_state in enumerate(models):\n",
    "        model_path = os.path.join(ensemble_path, f\"model_fold_{i+1}.pth\")\n",
    "        torch.save(model_state, model_path)\n",
    "    \n",
    "\n",
    "# Updated ensemble models data structure\n",
    "ensemble_models_data = {\n",
    "    'model_states': best_models,\n",
    "    'model_config': {\n",
    "        'seq_length': seq_length,\n",
    "        'num_features': X_feat.shape[1]  # Now 7 features\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'scalers': scalers,  # Dictionary containing all scalers\n",
    "        'participant_encoder': participant_encoder,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save ensemble model\n",
    "with open('feet_ensemble_models.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_models_data, f)\n",
    "print(\"Ensemble model has been saved to 'feet_ensemble_models_original data.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614f7ee-9d26-4c95-acad-e54146e5e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ensemble_models():\n",
    "    with open('feet_ensemble_models.pkl', 'rb') as f:\n",
    "        ensemble_data = pickle.load(f)\n",
    "    return ensemble_data\n",
    "\n",
    "# Integrated prediction function\n",
    "def ensemble_predict_custom(start_index, length, X_time, X_feat, y, tracker_info=None):\n",
    "    \"\"\"\n",
    "    Make predictions using the ensemble model on a subset of data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_index : int\n",
    "        Starting index for the prediction subset\n",
    "    length : int\n",
    "        Number of samples to predict\n",
    "    X_time : ndarray\n",
    "        Time series features\n",
    "    X_feat : ndarray\n",
    "        Additional features\n",
    "    y : ndarray\n",
    "        Ground truth targets\n",
    "    tracker_info : int, optional\n",
    "        Tracker ID to filter data (1, 2, or 3). If None, uses all data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_original : ndarray\n",
    "        Ensemble predictions in original scale\n",
    "    y_original : ndarray\n",
    "        Ground truth in original scale\n",
    "    vive_original : ndarray\n",
    "        Original VIVE data in original scale\n",
    "    \"\"\"\n",
    "    # Load ensemble model data\n",
    "    ensemble_data = load_ensemble_models()\n",
    "    scalers = ensemble_data['preprocessing']['scalers']\n",
    "    \n",
    "    # Filter data by tracker if specified\n",
    "    if tracker_info is not None:\n",
    "        # Extract tracker height feature from X_feat (3rd column, index 2)\n",
    "        # Need to inverse transform to get original tracker values\n",
    "        height_original = scalers['height'].inverse_transform(X_feat[:, 2].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Round to integer to handle floating point precision issues\n",
    "        height_rounded = np.round(height_original).astype(int)\n",
    "        \n",
    "        # Filter indices for the specified tracker\n",
    "        tracker_mask = (height_rounded == tracker_info)\n",
    "        tracker_indices = np.where(tracker_mask)[0]\n",
    "        \n",
    "        print(f\"Filtering data for Tracker {tracker_info}...\")\n",
    "        print(f\"Unique values (after rounding): {np.unique(height_rounded)}\")\n",
    "        \n",
    "        if len(tracker_indices) == 0:\n",
    "            print(f\"Error: No data found for Tracker {tracker_info}\")\n",
    "            print(f\"Available Tracker values: {np.unique(height_rounded)}\")\n",
    "            return None\n",
    "        \n",
    "        # Use filtered data\n",
    "        X_time = X_time[tracker_mask]\n",
    "        X_feat = X_feat[tracker_mask]\n",
    "        y = y[tracker_mask]\n",
    "        \n",
    "        print(f\"Successfully filtered Tracker {tracker_info} data: {len(X_time)} samples\")\n",
    "    \n",
    "    # Validate index range\n",
    "    end_index = min(start_index + length, len(X_time))\n",
    "    if start_index >= len(X_time) or start_index < 0:\n",
    "        print(f\"Error: Start index out of range [0, {len(X_time)-1}]\")\n",
    "        return None\n",
    "    \n",
    "    # Extract data subset\n",
    "    X_time_subset = X_time[start_index:end_index]\n",
    "    X_feat_subset = X_feat[start_index:end_index]\n",
    "    y_subset = y[start_index:end_index]\n",
    "    \n",
    "    # Create data loader\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_time_subset, dtype=torch.float32),\n",
    "        torch.tensor(X_feat_subset, dtype=torch.float32)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    \n",
    "    # Make predictions using all models in the ensemble\n",
    "    all_predictions = []\n",
    "    \n",
    "    for model_state in ensemble_data['model_states']:\n",
    "        model = AdvancedTransformerModel(\n",
    "            ensemble_data['model_config']['seq_length'],\n",
    "            num_features=ensemble_data['model_config']['num_features']\n",
    "        ).to(device)\n",
    "        model.load_state_dict(model_state)\n",
    "        model.eval()\n",
    "        \n",
    "        fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_time, batch_X_feat in test_loader:\n",
    "                batch_X_time = batch_X_time.to(device)\n",
    "                batch_X_feat = batch_X_feat.to(device)\n",
    "                output = model(batch_X_time, batch_X_feat)\n",
    "                fold_preds.append(output.cpu().numpy())\n",
    "        \n",
    "        fold_preds = np.vstack(fold_preds)\n",
    "        all_predictions.append(fold_preds)\n",
    "    \n",
    "    # Average predictions across all models\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    predictions_original = scalers['vicon_xyz'].inverse_transform(ensemble_predictions)\n",
    "    y_original = scalers['vicon_xyz'].inverse_transform(y_subset)\n",
    "    vive_original = scalers['vive_xyz'].inverse_transform(X_time_subset[:, -1])\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    ensemble_mse = np.mean((predictions_original - y_original) ** 2)\n",
    "    ensemble_rmse = np.sqrt(ensemble_mse)\n",
    "    ensemble_mae = np.mean(np.abs(predictions_original - y_original))\n",
    "    \n",
    "    vive_mse = np.mean((vive_original - y_original) ** 2)\n",
    "    vive_rmse = np.sqrt(vive_mse)\n",
    "    vive_mae = np.mean(np.abs(vive_original - y_original))\n",
    "    \n",
    "    rmse_improvement = ((vive_rmse - ensemble_rmse) / vive_rmse) * 100\n",
    "    mae_improvement = ((vive_mae - ensemble_mae) / vive_mae) * 100\n",
    "\n",
    "    # Add tracker information to output\n",
    "    tracker_label = f\" (Tracker {tracker_info})\" if tracker_info is not None else \"\"\n",
    "    print(f\"\\nPrediction results for index range {start_index}-{end_index-1}{tracker_label}:\")\n",
    "    print(f\"Original VIVE vs Vicon: RMSE={vive_rmse:.4f}, MAE={vive_mae:.4f}\")\n",
    "    print(f\"Ensemble model vs Vicon: RMSE={ensemble_rmse:.4f}, MAE={ensemble_mae:.4f}\")\n",
    "    print(f\"Improvement: RMSE {rmse_improvement:+.2f}%, MAE {mae_improvement:+.2f}%\")\n",
    "    \n",
    "    # Visualization - Updated for 3D XYZ data\n",
    "    sample_size = min(2000, len(predictions_original))\n",
    "    \n",
    "    # Add tracker information to plot titles\n",
    "    title_suffix = f\" - Tracker {tracker_info}\" if tracker_info is not None else \"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot each XYZ dimension separately\n",
    "    for i, dim in enumerate(['X', 'Y', 'Z']):\n",
    "        plt.subplot(3, 2, i*2+1)\n",
    "        plt.plot(y_original[:sample_size, i], label=f'Actual Vicon {dim}', linewidth=2)\n",
    "        plt.plot(predictions_original[:sample_size, i], label=f'Predicted {dim}', alpha=0.8)\n",
    "        plt.plot(vive_original[:sample_size, i], label=f'Original VIVE {dim}', alpha=0.7)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel(f'{dim} Value')\n",
    "        plt.title(f'{dim} Dimension Time Series{title_suffix}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(3, 2, i*2+2)\n",
    "        plt.scatter(y_original[:sample_size, i], predictions_original[:sample_size, i], \n",
    "                   alpha=0.5, s=1, label='Ensemble')\n",
    "        plt.scatter(y_original[:sample_size, i], vive_original[:sample_size, i], \n",
    "                   alpha=0.5, s=1, color='orange', label='VIVE')\n",
    "        plt.plot([y_original[:sample_size, i].min(), y_original[:sample_size, i].max()], \n",
    "                [y_original[:sample_size, i].min(), y_original[:sample_size, i].max()], 'r--')\n",
    "        plt.xlabel(f'Actual Vicon {dim}')\n",
    "        plt.ylabel(f'Predicted {dim}')\n",
    "        plt.title(f'{dim} Dimension Correlation{title_suffix}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Overall error distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for i, dim in enumerate(['X', 'Y', 'Z']):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        ensemble_errors = predictions_original[:, i] - y_original[:, i]\n",
    "        vive_errors = vive_original[:, i] - y_original[:, i]\n",
    "        \n",
    "        plt.hist(ensemble_errors, bins=30, alpha=0.7, \n",
    "                label=f'Ensemble (std: {np.std(ensemble_errors):.4f})', density=True)\n",
    "        plt.hist(vive_errors, bins=30, alpha=0.7, \n",
    "                label=f'VIVE (std: {np.std(vive_errors):.4f})', density=True)\n",
    "        plt.xlabel(f'{dim} Error')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'{dim} Error Distribution{title_suffix}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print per-dimension statistics\n",
    "    print(f\"\\nPer-Dimension Statistics:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    for i, dim in enumerate(['X', 'Y', 'Z']):\n",
    "        ensemble_rmse_dim = np.sqrt(np.mean((predictions_original[:, i] - y_original[:, i]) ** 2))\n",
    "        vive_rmse_dim = np.sqrt(np.mean((vive_original[:, i] - y_original[:, i]) ** 2))\n",
    "        improvement_dim = ((vive_rmse_dim - ensemble_rmse_dim) / vive_rmse_dim) * 100\n",
    "        \n",
    "        print(f\"{dim} Dimension:\")\n",
    "        print(f\"  VIVE RMSE:     {vive_rmse_dim:.4f}\")\n",
    "        print(f\"  Ensemble RMSE: {ensemble_rmse_dim:.4f}\")\n",
    "        print(f\"  Improvement:   {improvement_dim:+.2f}%\")\n",
    "    \n",
    "    return predictions_original, y_original, vive_original\n",
    "\n",
    "# Test the prediction function\n",
    "predictions, actual, vive = ensemble_predict_custom(\n",
    "    start_index=100000,\n",
    "    length=1000,\n",
    "    X_time=X_time,\n",
    "    X_feat=X_feat,\n",
    "    y=y,\n",
    "    tracker_info=3  # Tracker: 1, 2, or 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cedba-cbc2-4822-a9cf-00c3d7305dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
